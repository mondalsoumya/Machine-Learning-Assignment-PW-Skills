{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Supervised Classification: Decision Trees, SVM, and Naive Bayes**"
      ],
      "metadata": {
        "id": "LQC_m1Lc_vI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 1 : What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Information Gain** measures how much uncertainty (impurity) is reduced in the target variable after splitting a dataset based on a particular feature. It is most often calculated using entropy as the impurity measure.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "Information Gain=Entropy (before split) - Weighted Entropy (after split)\n",
        "\n",
        "Where:\n",
        "\n",
        "Entropy(before split) = impurity of the parent node\n",
        "\n",
        "Weighted Entropy(after split) = sum of impurities of the child nodes, weighted by their size\n",
        "\n",
        "**How it's used in Decision Trees:**\n",
        "1. Calculate entropy of the current dataset (the parent node).\n",
        "\n",
        "2. For each feature:\n",
        "\n",
        "- Split the data based on that feature.\n",
        "\n",
        "- Calculate the weighted average entropy of the resulting subsets.\n",
        "\n",
        "- Compute Information Gain for that feature.\n",
        "\n",
        "3. Choose the feature with the highest Information Gain — it gives the most reduction in impurity.\n",
        "\n",
        "4. Repeat the process recursively for child nodes."
      ],
      "metadata": {
        "id": "hnhgGCq0Ah8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| **Aspect**             | **Gini Impurity**                                                            | **Entropy**                                                         |\n",
        "| ---------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------- |\n",
        "| **Formula**            | ( Gini = 1 - ∑ p^2_i )                                                    | ( Entropy = - ∑ p_i \\log_2(p_i) )                                |\n",
        "| **Meaning**            | Measures the probability of misclassifying a randomly chosen element         | Measures the level of disorder or uncertainty in the dataset        |\n",
        "| **Value Range**        | 0 (pure) → 0.5 (for 2 classes, maximum impurity)                             | 0 (pure) → 1 (for 2 classes, maximum impurity)                      |\n",
        "| **When = 0**           | Node is pure (only one class present)                                        | Node is pure (only one class present)                               |\n",
        "| **Computation Speed**  | Faster (no logarithm calculation)                                            | Slightly slower (involves log₂)                                     |\n",
        "| **Behavior**           | Biased toward the largest class (more sensitive to class imbalance)      | More theoretically sound from an information theory perspective |\n",
        "| **Preferred By**       | CART algorithm (Classification and Regression Trees)                     | ID3, C4.5, C5.0 algorithms                                      |\n",
        "| **Splitting Tendency** | Prefers features that separate classes more clearly (larger class dominance) | Prefers features that provide maximum information gain          |\n"
      ],
      "metadata": {
        "id": "upASFjERArqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3:What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Pre-Pruning (Early Stopping)**\n",
        "\n",
        "- The tree's growth is **stopped early** based on certain conditions **before** it becomes overly complex.\n",
        "\n",
        "- Common stopping criteria:\n",
        "\n",
        "  - Maximum depth `(max_depth)`\n",
        "  - Minimum samples to split `(min_samples_split)`\n",
        "  - Minimum samples in a leaf `(min_samples_leaf)`\n",
        "  - Minimum impurity decrease\n",
        "  \n",
        "- **Practical advantage:** Saves computation time and prevents overfitting by keeping the model simpler from the start.\n"
      ],
      "metadata": {
        "id": "SyXeTee5A2Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).**\n",
        "\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "T8KSA0aJA98y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset (Iris dataset for example)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Model Accuracy:\", clf.score(X_test, y_test))\n",
        "\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68o5LuIsPxZA",
        "outputId": "fbfc7e38-7cdb-4192-aeb7-09a5495bfd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 5: What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Support Vector Machine (SVM)** is a powerful supervised machine learning algorithm primarily used for **classification**, though it can also handle regression tasks. Its main goal is to find the best boundary (called a **hyperplane**) that separates different classes in your data.\n",
        "\n",
        "Think of it like drawing a line on a 2D plot to separate red dots from blue dots—but it works in much higher dimensions too."
      ],
      "metadata": {
        "id": "YvxXzlrkBH2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 6: What is the Kernel Trick in SVM?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The **kernel Trick** is a clever mathematical shortcut that lets SVMs handle non-linearly separable data without explicitly transforming it into higher dimensions.\n",
        "\n",
        "**Why is this needed?**\n",
        "\n",
        "Imagine your data points can't be separated by a straight line in 2D, but if you were to “lift” them into 3D space, they would be separable by a flat plane. The problem is, explicitly calculating those new coordinates in high-dimensional space is often computationally expensive or impossible.\n",
        "\n",
        "The kernel trick **avoids the heavy lifting** by computing the dot product between points as if they were already transformed into that high-dimensional space — **without actually calculating their coordinates in that space.**\n",
        "\n",
        "**How it works:**\n",
        "- SVM decision functions rely on dot products between feature vectors.\n",
        "- A kernel function K(x,y)\n",
        " computes this dot product **in some transformed feature space.**\n",
        "- By plugging in the kernel function, SVM can operate as if the data were mapped into a higher-dimensional space, **enabling non-linear separation**."
      ],
      "metadata": {
        "id": "BVGSc_meBN-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.**\n",
        "\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "p9pQeBLMBUm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create two SVM classifiers: one with Linear kernel, one with RBF kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train both models\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"SVM Classifier with Linear Kernel - Accuracy:\", round(acc_linear, 4))\n",
        "print(\"SVM Classifier with RBF Kernel    - Accuracy:\", round(acc_rbf, 4))\n",
        "\n",
        "# Compare which performed better\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"\\n Linear Kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"\\n RBF Kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\n Both kernels performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlHnGIztShBq",
        "outputId": "6312ca66-fd6c-440b-f872-d829db714ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier with Linear Kernel - Accuracy: 1.0\n",
            "SVM Classifier with RBF Kernel    - Accuracy: 0.8056\n",
            "\n",
            " Linear Kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Naïve Bayes is a **probabilistic machine learning algorithm** used mainly for classification tasks. It's based on **Bayes' Theorem**, which helps you update the probability estimate for a hypothesis as more evidence comes in.\n",
        "\n",
        "In simple terms, it predicts the class of a data point by calculating the probability that it belongs to each class and then picking the class with the highest probability.\n",
        "\n",
        "**How does it work?**\n",
        "\n",
        "It looks at the features (attributes) of your data.\n",
        "Uses Bayes' theorem to calculate the probability of the data belonging to each class.\n",
        "Assigns the class with the **highest posterior probability**.\n",
        "\n",
        "**Why is it called \"Naïve\"?**\n",
        "\n",
        "The \"naïve\" part comes from a **strong assumption it makes**:\n",
        "\n",
        "- It assumes all features are independent of each other, given the class label.\n",
        "- In reality, features often influence each other (they're correlated), but Naïve Bayes ignores this and treats each feature as if it stands alone.\n",
        "\n",
        "This assumption is what makes it \"naïve\" — it simplifies the math drastically, making the algorithm fast and efficient, even if the assumption isn't perfectly true."
      ],
      "metadata": {
        "id": "wQkyDrCrBdxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Gaussian Naïve Bayes**: ⬇\n",
        "\n",
        "Gaussian Naïve Bayes is used when the features are **continuous numerical values**, such as height, weight, or temperature.\n",
        "It assumes that the data for each class follows a **normal (Gaussian) distribution**.\n",
        "For example, in the Iris dataset, where the features are continuous measurements of flower parts, Gaussian NB works very well.\n",
        "It calculates the probability of a feature value belonging to a class using the mean and variance of that class’s Gaussian distribution.\n",
        "\n",
        "**Multinomial Naïve Bayes** ⬇\n",
        "\n",
        "Multinomial Naïve Bayes is designed for **discrete count data** — that is, features that represent how many times something occurs.\n",
        "It is commonly used in **text classification problems**, such as spam detection or document categorization, where features are **word counts or term frequencies**.\n",
        "It assumes that the features are counts drawn from a **Multinomial distribution** (non-negative integers).\n",
        "For example, if a document contains a word five times, that count is used directly in the model.\n",
        "\n",
        "\n",
        "**Bernoulli Naïve Bayes** ⬇\n",
        "\n",
        "Bernoulli Naïve Bayes is used when the features are **binary (either 0 or 1)**, indicating the **presence or absence** of something.\n",
        "In text classification, instead of counting how many times a word appears, we only record whether a word appears at all.\n",
        "For instance, if a word occurs at least once, its feature value is 1; otherwise, it is 0.\n",
        "This model assumes a **Bernoulli distribution** and is particularly useful in tasks like sentiment analysis or binary feature-based classification."
      ],
      "metadata": {
        "id": "xbAjL8uOVoWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 10: Breast Cancer Dataset**\n",
        "\n",
        "**Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.**\n",
        "\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "2-UJcuN6BuzO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDq60STq_sV3",
        "outputId": "504894fa-8c68-480d-bbf2-1d2c90189bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_test, y_pred, target_names=data.target_names)\n",
        "print(report)"
      ]
    }
  ]
}